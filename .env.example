# -------------------------
# Core
# -------------------------
NODE_ENV=development
PORT=4000

# -------------------------
# OpenSearch
# -------------------------
OPENSEARCH_URL=http://localhost:19200
# OPENSEARCH_USERNAME=admin
# OPENSEARCH_PASSWORD=admin
# For self-signed local dev certs set to false
OPENSEARCH_SSL_REJECT_UNAUTHORIZED=true

# Index naming overrides (optional)
MEMORA_METRICS_INDEX=mem-metrics
MEMORA_SEMANTIC_INDEX=mem-semantic
MEMORA_FACTS_INDEX=mem-facts
MEMORA_EPI_PREFIX=mem-episodic-
MEMORA_IDEMP_INDEX=mem-idempotency

# -------------------------
# Embeddings
# -------------------------
# If set, Memora will POST to this endpoint with {texts, dim}
# and expect {vectors: number[][]}
# If unset, a deterministic hash-based fallback is used
# EMBEDDING_ENDPOINT=http://localhost:8080/embed  # optional; if unset, deterministic local fallback is used
# EMBEDDING_API_KEY=changeme
MEMORA_EMBED_DIM=384
MEMORA_EMBED_TIMEOUT_MS=8000
MEMORA_EMBED_RETRIES=3

# -------------------------
# OpenSearch ML Pipelines (optional)
# -------------------------
# When using OpenSearch ML Commons to generate embeddings via pipelines:
# - Set MEMORA_EMBED_PROVIDER=opensearch_pipeline
# - Ensure an ONNX model is registered/deployed (e.g., MiniLM-L6 384-dim)
# - Create an ingest pipeline with text_embedding processor referencing your model_id
# - Optionally attach the ingest pipeline as the index default_pipeline
#
# Provider selection
MEMORA_EMBED_PROVIDER=opensearch_pipeline
#
# Model configuration (defaults for local/dev)
# OPENSEARCH_ML_MODEL_NAME=huggingface/sentence-transformers/all-MiniLM-L6-v2
# OPENSEARCH_ML_MODEL_VERSION=1.0.2
# OPENSEARCH_ML_MODEL_FORMAT=ONNX
# OPENSEARCH_ML_MODEL_ID=XXXXXXXXXXXX    # required to create the ingest pipeline (deployed ML model id)
# MEMORA_OS_APPLY_DEV_ML_SETTINGS=false  # set true to apply dev ML cluster settings (single-node convenience)
# MEMORA_OS_AUTO_REGISTER_MODEL=false     # (dev-only) future convenience flag to auto-register/deploy a default model when OPENSEARCH_ML_MODEL_ID is unset. Not implemented yet; see README for status.
#
# Pipelines
# MEMORA_OS_INGEST_PIPELINE_NAME=mem-text-embed
# MEMORA_OS_TEXT_SOURCE_FIELD=text      # source field containing plain text
# MEMORA_OS_EMBED_FIELD=embedding       # destination field to store embedding vector
# MEMORA_OS_DEFAULT_PIPELINE_ATTACH=false
#
# Search pipelines (optional)
# Set a search pipeline to process queries server-side (e.g., embed queries, rewrite, rerank).
# MEMORA_OS_SEARCH_PIPELINE_NAME=mem-search
# MEMORA_OS_SEARCH_DEFAULT_PIPELINE_ATTACH=false
# Provide the full pipeline body as single-line JSON. Example shown uses a trivial filter processor.
# For ML inference or other processors, see:
# https://docs.opensearch.org/latest/search-plugins/search-pipelines/search-processors/
# MEMORA_OS_SEARCH_PIPELINE_BODY_JSON={"request_processors":[{"filter_query":{"description":"example","query":{"match_all":{}}}}],"response_processors":[]}
#
# Example A (request-time embedding via ml_inference request processor):
# MEMORA_OS_SEARCH_PIPELINE_BODY_JSON={"request_processors":[{"ml_inference":{"description":"Embed query","model_id":"YOUR_EMBED_MODEL_ID","input_map":{"text":"params.query_text"},"output_map":{"vector":"ctx.query_vector"}}}],"response_processors":[]}
# Notes:
# - Provide a query_text parameter to the pipeline per OpenSearch docs.
# - Use ctx.query_vector in the neural/kNN query as supported by your version.
#
# Example B (response-time reranking):
# MEMORA_OS_SEARCH_PIPELINE_BODY_JSON={"request_processors":[],"response_processors":[{"rerank":{"description":"Cross-encoder rerank","model_id":"YOUR_RERANK_MODEL_ID","top_k":50}}]}
#
# Vector dim alignment
# MEMORA_OS_AUTOFIX_VECTOR_DIM=false
# Note: MiniLM-L6 ONNX outputs 384-dim vectors. If using that model, set:
# MEMORA_EMBED_DIM=384
# and ensure index mappings match (or enable MEMORA_OS_AUTOFIX_VECTOR_DIM=true).

# -------------------------
# Reranker (optional)
# -------------------------
# If set, Memora will POST to this endpoint with {query, candidates}
# and expect {scores: number[]}
RERANK_ENDPOINT=http://localhost:8081/rerank
RERANK_API_KEY=changeme
RERANK_TIMEOUT_MS=1500
RERANK_MAX_RETRIES=2

# -------------------------
# Evaluation (optional)
# -------------------------
# Mirror eval.log entries into the episodic log
MEMORA_EVAL_EPISODIC_MIRROR=false

# -------------------------
# Memory defaults
# -------------------------
MEMORA_DEFAULT_BUDGET=12
MEMORA_RERANK_ENABLED=false
# Max number of semantic chunks to upsert per memory.write (backpressure cap; default 64)
MEMORA_WRITE_MAX_CHUNKS=64

# -------------------------
# Logging / Debug
# -------------------------
DEBUG=memora:*
LOG_LEVEL=info
