# -------------------------
# Core
# -------------------------
NODE_ENV=development
PORT=4000

# -------------------------
# OpenSearch
# -------------------------
# Local dev: docker-compose exposes OpenSearch at 9200 (Dashboards at 5601).
# Use 19200/15601 only if you remap ports (older docs may reference 19200/15601).
OPENSEARCH_URL=http://localhost:9200
# OPENSEARCH_USERNAME=admin
# OPENSEARCH_PASSWORD=admin
# For self-signed local dev certs set to false
OPENSEARCH_SSL_REJECT_UNAUTHORIZED=true

# OpenSearch client tuning (preferred; falls back to legacy env)
# MEMORA_OS_CLIENT_MAX_RETRIES=3
# MEMORA_OS_CLIENT_TIMEOUT_MS=10000
# Legacy (still supported for back-compat):
# MEMORA_OS_MAX_RETRIES=3
# MEMORA_OS_REQUEST_TIMEOUT_MS=10000

# Index naming overrides (optional)
MEMORA_METRICS_INDEX=mem-metrics
MEMORA_SEMANTIC_INDEX=mem-semantic
MEMORA_FACTS_INDEX=mem-facts
MEMORA_EPI_PREFIX=mem-episodic-
MEMORA_IDEMP_INDEX=mem-idempotency

# -------------------------
# Embeddings
# -------------------------
# If set, Memora will POST to this endpoint with {texts, dim}
# and expect {vectors: number[][]}
# If unset, a deterministic hash-based fallback is used
# EMBEDDING_ENDPOINT=http://localhost:8080/embed  # optional; if unset, deterministic local fallback is used
# EMBEDDING_API_KEY=changeme
MEMORA_EMBED_DIM=384
MEMORA_EMBED_TIMEOUT_MS=8000
MEMORA_EMBED_RETRIES=3

# -------------------------
# OpenSearch ML Pipelines (optional)
# -------------------------
# When using OpenSearch ML Commons to generate embeddings via pipelines:
# - Set MEMORA_EMBED_PROVIDER=opensearch_pipeline
# - Ensure an ONNX model is registered/deployed (e.g., MiniLM-L6 384-dim)
# - Create an ingest pipeline with text_embedding processor referencing your model_id
# - Optionally attach the ingest pipeline as the index default_pipeline
#
# Provider selection
MEMORA_EMBED_PROVIDER=opensearch_pipeline
#
# Model configuration (defaults for local/dev)
# OPENSEARCH_ML_MODEL_NAME=huggingface/sentence-transformers/all-MiniLM-L6-v2
# OPENSEARCH_ML_MODEL_VERSION=1.0.2
# OPENSEARCH_ML_MODEL_FORMAT=ONNX
# OPENSEARCH_ML_MODEL_ID=XXXXXXXXXXXX    # required to create the ingest pipeline unless MEMORA_OS_AUTO_REGISTER_MODEL=true (dev-only)
# MEMORA_OS_APPLY_DEV_ML_SETTINGS=false  # set true to apply dev ML cluster settings (single-node convenience)
# MEMORA_OS_AUTO_REGISTER_MODEL=false     # (dev-only) when true and OPENSEARCH_ML_MODEL_ID is unset, auto-register/deploy default ONNX model for dev. Do not enable in CI/prod.
# MEMORA_OS_MODEL_ID_CACHE_FILE=.memora/model_id  # dev-only cache file path. Persist a resolved model_id for faster subsequent runs.
#
# Pipelines
# MEMORA_OS_INGEST_PIPELINE_NAME=mem-text-embed
# MEMORA_OS_TEXT_SOURCE_FIELD=text      # source field containing plain text
# MEMORA_OS_EMBED_FIELD=embedding       # destination field to store embedding vector
# MEMORA_OS_DEFAULT_PIPELINE_ATTACH=false
#
# Search pipelines (optional)
# Set a search pipeline to process queries server-side (e.g., embed queries, rewrite, rerank).
# MEMORA_OS_SEARCH_PIPELINE_NAME=mem-search
# MEMORA_OS_SEARCH_DEFAULT_PIPELINE_ATTACH=false
# Provide the full pipeline body as single-line JSON. Example shown uses a trivial filter processor.
# For ML inference or other processors, see:
# https://docs.opensearch.org/latest/search-plugins/search-pipelines/search-processors/
# MEMORA_OS_SEARCH_PIPELINE_BODY_JSON={"request_processors":[{"filter_query":{"description":"example","query":{"match_all":{}}}}],"response_processors":[]}
#
# Example A (request-time embedding via ml_inference request processor):
# MEMORA_OS_SEARCH_PIPELINE_BODY_JSON={"request_processors":[{"ml_inference":{"description":"Embed query","model_id":"YOUR_EMBED_MODEL_ID","input_map":{"text":"params.query_text"},"output_map":{"vector":"ctx.query_vector"}}}],"response_processors":[]}
# Notes:
# - Provide a query_text parameter to the pipeline per OpenSearch docs.
# - Use ctx.query_vector in the neural/kNN query as supported by your version.
#
# Example B (response-time reranking):
# MEMORA_OS_SEARCH_PIPELINE_BODY_JSON={"request_processors":[],"response_processors":[{"rerank":{"description":"Cross-encoder rerank","model_id":"YOUR_RERANK_MODEL_ID","top_k":50}}]}
#
# Vector dim alignment
# MEMORA_OS_AUTOFIX_VECTOR_DIM=false
# Note: MiniLM-L6 ONNX outputs 384-dim vectors. If using that model, set:
# MEMORA_EMBED_DIM=384
# and ensure index mappings match (or enable MEMORA_OS_AUTOFIX_VECTOR_DIM=true).

# -------------------------
# Reranker (optional)
# -------------------------
# If set, Memora will POST to this endpoint with {query, candidates}
# and expect {scores: number[]}
RERANK_ENDPOINT=http://localhost:8081/rerank
RERANK_API_KEY=changeme
RERANK_TIMEOUT_MS=1500
RERANK_MAX_RETRIES=2

# OpenSearch ML cross-encoder rerank (takes precedence if configured)
# OPENSEARCH_ML_RERANK_MODEL_ID=YOUR_RERANK_MODEL_ID
# OPENSEARCH_ML_RERANK_TIMEOUT_MS=1500

# Cohere cross-encoder adapter (scripts/dev/reranker_cohere_server.ts)
# Requires: COHERE_API_KEY. Optional: COHERE_RERANK_MODEL, COHERE_API_BASE, RERANK_PORT
# Start the adapter:
#   export COHERE_API_KEY=sk_...
#   npm run dev:reranker:cohere
# Example run (separate terminal):
#   export RERANK_ENDPOINT=http://localhost:8081/rerank
#   export MEMORA_RERANK_ENABLED=true
#   bash benchmarks/runners/run_longmemeval.sh --variant C --seed 42 --dataset benchmarks/LongMemEval/data/longmemeval_s.json --out benchmarks/reports/longmemeval.C.s.42.jsonl --tag "memora+cohere-rerank"
#
# COHERE_API_KEY=changeme
# COHERE_RERANK_MODEL=rerank-english-v3.0
# COHERE_API_BASE=https://api.cohere.com/v1
# RERANK_PORT=8081

# -------------------------
# Retrieval Tuning Overrides (optional)
# -------------------------
# Load alternate retrieval YAML or overlay overrides for sweeps.
# MEMORA_RETRIEVAL_CONFIG_PATH=config/retrieval.yaml
# MEMORA_RETRIEVAL_OVERRIDES_FILE=overrides.json
# MEMORA_RETRIEVAL_OVERRIDES_JSON={"stages":{"semantic":{"top_k":150},"ann_candidates":200},"fusion":{"rrf_k":60},"diversity":{"lambda":0.8},"rerank":{"enabled":true,"max_candidates":32}}
# Notes:
# - File overrides are applied first; JSON overrides take precedence.
# - Overrides are deep-merged; arrays/scalars replace base values.
# - Useful for A/B/C runs without editing config/retrieval.yaml.
#
# -------------------------
# Evaluation (optional)
# -------------------------
# Mirror eval.log entries into the episodic log
MEMORA_EVAL_EPISODIC_MIRROR=false

# -------------------------
# Memory defaults
# -------------------------
MEMORA_DEFAULT_BUDGET=12
# Rerank gating precedence:
# - If MEMORA_RERANK_ENABLED is set, it overrides retrieval.yaml: rerank.enabled
# - If MEMORA_RERANK_ENABLED is unset, retrieval.yaml controls rerank
# Default: leave this unset to use retrieval.yaml
# MEMORA_RERANK_ENABLED=true
# Max number of semantic chunks to upsert per memory.write (backpressure cap; default 64)
MEMORA_WRITE_MAX_CHUNKS=64

# -------------------------
# Logging / Debug
# -------------------------
DEBUG=memora:*
LOG_LEVEL=info
