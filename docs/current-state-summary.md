# Memora Retrieval Performance - Current State

**Date**: October 2025
**Last Updated**: 2025-10-22
**Status**: Lexical optimization complete, reranking tested

---

## Current Performance

### LongMemEval Benchmark Results

| Configuration | Accuracy | Details |
|---------------|----------|---------|
| **Baseline (lexical-only)** | **76%** | 38/50 questions correct (seed 42) |
| With SageMaker reranking | 77% | +1% improvement, 13x latency cost |

**Achievement**: ðŸŽ¯ **75%+ accuracy target reached** with lexical-only approach

---

## Completed Work

### 1. Benchmark Framework Improvements âœ…
**Status**: COMPLETE
**Documentation**: `docs/benchmark-framework-improvements.md`

**Implemented**:
- âœ… Priority 1: Explicit dataset parameter (required, no auto-selection)
- âœ… Priority 2: Metadata embedding in output files (provenance tracking)
- âœ… Priority 3: Standardized result parsing utility (`scripts/parse_benchmark_results.ts`)
- âœ… Priority 4: Descriptive dataset naming with v1 convention
- âœ… Priority 5: Validation and pre-flight checks

**Impact**: Eliminated confusion, enabled reproducible comparisons, self-documenting results

---

### 2. Lexical Search Optimization âœ…
**Status**: COMPLETE - Target achieved
**Documentation**: `docs/lexical-improvements-roadmap.md`

**Feature Stack (all enabled)**:
1. âœ… Query expansion for temporal terms (Session 3)
2. âœ… Enhanced entity extraction with date normalization, temporal units, disambiguation, acronyms (Session 4)
3. âœ… Dynamic boosting based on query type classification (Session 6)
4. âœ… Cross-field matching for better entity coordination (Sessions 7-8, **+6% improvement**)

**Results**:
- Started: 68% baseline (Variant B, 50 questions)
- Cross-fields improvement: 70% â†’ **76%** (+6%)
- **Target achieved**: 75%+ accuracy âœ…

**Key Findings**:
- Cross-field matching (`cross_fields` vs `best_fields`) provided largest single improvement (+6%)
- Query expansion, entity extraction, dynamic boosting each contributed incremental gains
- Lexical search with proper tuning can achieve strong performance without semantic search

---

### 3. BM25 Parameter Tuning âœ…
**Status**: COMPLETE
**Documentation**: `docs/bm25-tuning-guide.md`, `benchmarks/reports/bm25_tuning/`

**Approach**: Grid search over k1 (0.8-2.0) and b (0.4-0.8)

**Results**:
- Optimal: k1=1.2, b=0.6
- Impact: +2-3% overall accuracy improvement
- Current config: `config/retrieval.yaml` (k1=1.2, b=0.6)

---

### 4. Cross-Encoder Reranking âœ…
**Status**: COMPLETE - Integration working, modest gains
**Documentation**: `docs/reranking-integration-results.md`

**Implementation**:
- Service: `src/services/rerank.ts` (SageMaker, OpenSearch ML, or HTTP endpoint support)
- Integration: `src/routes/memory.ts:790-798` (positioned after fusion, before diversification)
- Model: BGE reranker-large (BAAI/bge-reranker-large)
- Default candidates: 50 (based on top-50 recall analysis)

**Results**:
- Baseline: 76% accuracy (38/50)
- With reranking: 77% accuracy (77/100 predictions)
- Improvement: **+1 percentage point**
- Latency impact: **13x increase** (15ms â†’ 201ms median)

**Decision**: âœ… **Keep integration but disabled by default**
- Code is production-ready and working correctly
- Can be enabled via `MEMORA_RERANK_ENABLED=true` environment variable
- Modest gains don't justify latency cost for most use cases
- Useful for latency-insensitive, precision-critical scenarios

---

### 5. Structured Temporal Reasoning âœ…
**Status**: COMPLETE - Tested, not recommended for deployment
**Documentation**: `benchmarks/services/temporal-reasoning.ts`

**Implementation**:
- Service: `benchmarks/services/temporal-reasoning.ts` (LLM-based event extraction + programmatic date calculations)
- Integration: `benchmarks/runners/longmemeval_driver.ts` (--structured-reasoning flag)
- Model: gpt-4o-mini for temporal extraction, then JavaScript Date math for ordering and duration calculations

**Results on 500q dataset**:
- Accuracy: **73.2%** (732/1000 correct)
- Baseline: **76%** (38/50 on 50q subset)
- **Degradation: -2.8% from baseline**

**Cost Analysis**:
- 2x LLM calls per question (extraction + answer)
- Mean tokens in: 13,424 (vs baseline ~7,000)
- Mean latency: 1,401ms per question
- MCP latency: memory.write 18.5ms, memory.retrieve 33.4ms

**Decision**: âŒ **Do NOT deploy**
- No accuracy improvement (actually worse)
- Doubles LLM API costs
- Potential causes: extraction errors, date parsing failures, context overflow from prepended analysis
- Temporal reasoning may not be the primary failure mode

---

### 6. Semantic Search (Dense Retrieval) ðŸ”„
**Status**: Infrastructure ready, disabled by default
**Branch**: `feature/re-enable-semantic`
**Documentation**: `docs/semantic-search-findings.md`, `docs/retrieval-optimization-summary.md`

**Implementation**:
- Embeddings: MiniLM-L6 (384-dim)
- Index: HNSW (ef_search=200, m=32, cosine similarity)
- Fusion: RRF (k=60) combining lexical + semantic

**Results on 11 Target Questions**:
- Result: 0/11 correct (same as lexical-only)
- Semantic similarity doesn't help with temporal reasoning queries
- MiniLM-L6 doesn't capture temporal relationships

**Status**: Infrastructure production-ready but disabled by default (no benefit for current failure modes)

---

## Current Configuration

### Enabled Features (Default)
- âœ… BM25 lexical search (k1=1.2, b=0.6)
- âœ… Query expansion for temporal terms
- âœ… Enhanced entity extraction (dates, numbers, temporal units, disambiguation, acronyms)
- âœ… Dynamic field boosting based on query type
- âœ… Cross-field matching (`cross_fields` mode)
- âœ… Phrase matching with shingles (2-word shingles for multi-word exact matching)
- âœ… Phrase boosting for extracted entities, dates, numbers (via match_phrase queries)

### Features NOT Implemented
- âŒ Term proximity boosting (span_near queries) - not implemented
- âŒ Named entity recognition (NER) specific boosting - using rule-based entity extraction instead
- âŒ Field-specific analyzers (light_english, keyword) - using default english analyzer for all fields

### Disabled Features (Available but Off)
- âŒ Semantic search (k-NN vectors) - no benefit for temporal reasoning
- âŒ Cross-encoder reranking - only +1%, 13x latency cost
- âŒ OpenSearch ML reranking plugin - not tested

### Configuration Files
- `config/retrieval.yaml` - BM25 params, feature flags
- `config/index-templates/mem-episodic.json` - Index settings
- Environment variables:
  - `MEMORA_RERANK_ENABLED=false` (default)
  - `MEMORA_SEMANTIC_ENABLED=false` (default)

---

## Key Insights

### Retrieval is Working Well
**Evidence from multiple approaches**:
1. Query expansion: Retrieved documents with dates/events
2. Semantic search: Retrieved relevant docs, same failure modes
3. Manual analysis: All failures had sufficient context (7000+ tokens average)

**Conclusion**: The bottleneck has shifted from **retrieval quality** to **LLM temporal reasoning**.

### Failure Analysis
**11 Consistently-Failing Questions** (from error-analysis-c43.md):

**Categories**:
1. **Date Ordering** (3/11): Can't determine month order (January before February)
2. **Numerical Calculations** (3/11): Wrong day/month counts, event counting errors
3. **Semantic Understanding** (4/11): Entity extraction errors, timeline interpretation mistakes

**Common Pattern**:
```
âœ… Query submitted: "Which device first, thermostat or router?"
âœ… Documents retrieved: "thermostat 2/10", "router January 15th"
âŒ LLM reasoning: "thermostat first" (WRONG - Jan before Feb)
```

---

## Recommendations

### Short-Term: Production-Ready State âœ…
**Current baseline is solid**:
- 76% accuracy with lexical-only search
- Fast retrieval (15ms median)
- No external dependencies (SageMaker, semantic index)
- Self-documenting benchmark framework

**Recommendation**: **Deploy current configuration to production**

---

### Medium-Term: LLM Reasoning Improvements

Since retrieval is working, focus on improving LLM temporal reasoning:

**Option A: Enhanced Prompting** (Not yet tested)
- Add explicit date ordering rules to system prompt
- Request step-by-step reasoning for temporal questions
- Extract dates/events to structured format before reasoning

**Option B: Structured Reasoning Pipeline** âŒ **TESTED - Not Effective**
- Result: **73.2%** on 500q (worse than 76% baseline)
- Implementation: `benchmarks/services/temporal-reasoning.ts`
- Issues: Extraction errors, context overflow, 2x LLM cost
- Not recommended for deployment

**Option C: Different LLM** (Not yet tested)
- Try GPT-4 Turbo, Claude 3 Opus, Gemini Pro
- Use chain-of-thought specialized models
- Consider fine-tuning on temporal QA

---

### Long-Term: When to Revisit Retrieval

**Enable Semantic Search** when:
- Corpus grows significantly (>10K documents)
- Paraphrase matching becomes valuable
- Non-temporal questions dominate failures
- Better embedding models available (E5, BGE with temporal reasoning)

**Enable Reranking** when:
- Latency budget allows (can tolerate 13x increase)
- Precision critical, every 1% matters
- Can use local cross-encoder (lower latency than SageMaker)
- Adaptive reranking implemented (only rerank when scores are close)

**Further BM25 Tuning** when:
- Document statistics change (length distribution, term frequency)
- Field-specific boost optimization shows promise
- New query patterns emerge

---

## Files and Documentation

### Core Implementation
- `src/routes/memory.ts` - Retrieval pipeline (episodicSearch, reranking integration)
- `src/services/entity-extraction.ts` - Enhanced entity extraction
- `src/services/query-classifier.ts` - Query type classification for dynamic boosting
- `src/services/query-expansion.ts` - Temporal query expansion
- `src/services/rerank.ts` - Cross-encoder reranking service

### Configuration
- `config/retrieval.yaml` - Feature flags, BM25 parameters
- `config/index-templates/mem-episodic.json` - Index settings
- `benchmarks/config/llm.json` - LLM configuration (temperature=0.0, seed support)
- `benchmarks/config/memora.json` - Benchmark-specific config

### Scripts and Tooling
- `scripts/parse_benchmark_results.ts` - Standardized result parsing and comparison
- `scripts/tune_bm25_quick.sh` - BM25 parameter tuning
- `benchmarks/runners/longmemeval_driver.ts` - Benchmark driver with metadata
- `benchmarks/runners/run_longmemeval.sh` - Benchmark runner with validation

### Documentation
- `docs/current-state-summary.md` - This file (high-level overview)
- `docs/benchmark-framework-improvements.md` - Benchmarking tooling improvements
- `docs/lexical-improvements-roadmap.md` - Detailed lexical optimization journey
- `docs/retrieval-optimization-summary.md` - Retrieval optimization findings
- `docs/reranking-integration-results.md` - Reranking experiment results
- `docs/bm25-tuning-guide.md` - BM25 parameter tuning methodology
- `docs/semantic-search-findings.md` - Dense retrieval analysis
- `docs/error-analysis-c43.md` - Failure categorization

### Test Results
- `benchmarks/reports/longmemeval.C.42.jsonl` - Baseline results (76%)
- `benchmarks/reports/longmemeval.C.43.with_rerank.jsonl` - Reranking results (77%)
- `benchmarks/reports/bm25_tuning/` - BM25 parameter sweep results
- `benchmarks/reports/boost_tuning/` - Field boost tuning results

---

## No Background Experiments Running

**Confirmed** (as of 2025-10-22):
- âœ… No background processes running (`ps aux` check)
- âœ… SageMaker reranking tests complete
- âœ… BM25 tuning complete
- âœ… Semantic search testing complete

All major retrieval optimization work is **COMPLETE**.

---

## Next Steps

### Immediate
1. âœ… Documentation updated to reflect current state
2. Consider deploying current configuration (76% accuracy, lexical-only)
3. Archive old/outdated documentation

### Future Work (If Desired)
1. **LLM reasoning improvements** (see Medium-Term Recommendations)
2. **Move to full 500-question dataset** for more comprehensive evaluation
3. **Multi-session performance analysis** (aggregation queries, preference queries)
4. **Field-specific analyzers** (if fine-grained text analysis becomes valuable)
5. **Proximity boosting** (if multi-entity coordination needs improvement)

---

## Success Metrics

âœ… **Target achieved**: 75%+ accuracy on LongMemEval (current: 76%)
âœ… **Reproducible benchmarks**: Metadata, seed control, deterministic results
âœ… **Fast retrieval**: 15ms median latency
âœ… **Production-ready**: No external dependencies for baseline performance
âœ… **Well-documented**: Comprehensive docs, clear decision rationale

**Recommendation**: Accept current state as production baseline, focus future work on LLM reasoning rather than retrieval.
